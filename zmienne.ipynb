{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c56bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytywanie danych z pliku: C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\pacz.json\n",
      "Przetwarzanie elementów...\n",
      "Znaleziono 19646 paczkomatów.\n",
      "Zapisywanie danych do pliku CSV: C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\paczkomaty.csv\n",
      "Dane zostały pomyślnie zapisane do C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\paczkomaty.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define paths to the JSON input and the output CSV\n",
    "json_file_path = r\"C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\pacz.json\"\n",
    "csv_folder_path = r\"C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\"\n",
    "csv_file_name = \"paczkomaty.csv\"\n",
    "csv_file_path = os.path.join(csv_folder_path, csv_file_name)\n",
    "\n",
    "# Load the JSON file\n",
    "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "parcel_lockers = []      # List to store parsed parcel locker data\n",
    "all_tag_keys = set()     # Set to collect all unique tag keys across lockers\n",
    "\n",
    "# Iterate through each element in the JSON\n",
    "for element in data.get('elements', []):\n",
    "    # Filter only parcel lockers\n",
    "    if element.get('tags', {}).get('amenity') == 'parcel_locker':\n",
    "        # Initialize locker info with ID and type\n",
    "        locker_data = {\n",
    "            'id': element.get('id'),\n",
    "            'type': element.get('type')\n",
    "        }\n",
    "\n",
    "        # Extract latitude and longitude\n",
    "        if element.get('type') == 'node':\n",
    "            # For 'node' types, coordinates are direct\n",
    "            locker_data['latitude'] = element.get('lat')\n",
    "            locker_data['longitude'] = element.get('lon')\n",
    "        else:\n",
    "            # For other types (e.g., 'way'), use 'center' if available\n",
    "            locker_data['latitude'] = element.get('lat')\n",
    "            locker_data['longitude'] = element.get('lon')\n",
    "            if 'center' in element:\n",
    "                locker_data['latitude'] = element['center'].get('lat')\n",
    "                locker_data['longitude'] = element['center'].get('lon')\n",
    "\n",
    "        # Extract all available tags (metadata) and collect tag keys\n",
    "        if 'tags' in element:\n",
    "            for key, value in element['tags'].items():\n",
    "                locker_data[key] = value\n",
    "                all_tag_keys.add(key)\n",
    "\n",
    "        # Add processed locker data to the list\n",
    "        parcel_lockers.append(locker_data)\n",
    "\n",
    "# Write results to CSV if any lockers were found\n",
    "if parcel_lockers:\n",
    "    base_headers = ['id', 'type', 'latitude', 'longitude']  # Basic columns\n",
    "    sorted_tag_keys = sorted(list(all_tag_keys))            # Sorted additional metadata columns\n",
    "\n",
    "    # Combine base headers with unique tag keys\n",
    "    final_headers = base_headers[:]\n",
    "    for key in sorted_tag_keys:\n",
    "        if key not in final_headers:\n",
    "            final_headers.append(key)\n",
    "\n",
    "    # Write to CSV\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=final_headers, extrasaction='ignore')\n",
    "        writer.writeheader()           # Write column headers\n",
    "        writer.writerows(parcel_lockers)  # Write each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba07e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytywanie danych z pliku: C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\paczkomaty.csv\n",
      "Przetworzono 19646 wierszy.\n",
      "Zapisywanie danych do finalnego pliku CSV: C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\final.csv\n",
      "Dane zostały pomyślnie zapisane do C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\final.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Define input and output file paths\n",
    "input_csv_file_path = r\"C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\paczkomaty.csv\"\n",
    "output_csv_folder_path = r\"C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\"\n",
    "output_csv_file_name = \"final.csv\"\n",
    "output_csv_file_path = os.path.join(output_csv_folder_path, output_csv_file_name)\n",
    "\n",
    "# Define the final columns we want in the output CSV\n",
    "final_columns = ['id', 'latitude', 'longitude', 'brand']\n",
    "final_data = []  # List to hold processed rows\n",
    "\n",
    "# Open and read the input CSV\n",
    "with open(input_csv_file_path, 'r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    if reader.fieldnames:  # Check if headers exist\n",
    "        for row in reader:\n",
    "            processed_row = {}\n",
    "            for col in final_columns:\n",
    "                # Get the value for each column we're interested in, or an empty string if missing\n",
    "                processed_row[col] = row.get(col, '') \n",
    "            final_data.append(processed_row)  # Add to the final list\n",
    "\n",
    "# Write the filtered data to the output CSV\n",
    "if final_data:\n",
    "    with open(output_csv_file_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=final_columns)\n",
    "        writer.writeheader()           # Write column headers\n",
    "        writer.writerows(final_data)   # Write each processed row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytywanie danych z pliku: C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\final.csv\n",
      "Przetworzono 19646 wierszy.\n",
      "Zapisywanie zmodyfikowanych danych do pliku CSV: C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\final1.csv\n",
      "Dane zostały pomyślnie zapisane do C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\final1.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Define input and output file paths\n",
    "input_csv_file_path = r\"C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\final.csv\"\n",
    "output_csv_folder_path = r\"C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\"\n",
    "output_csv_file_name = \"final1.csv\"\n",
    "output_csv_file_path = os.path.join(output_csv_folder_path, output_csv_file_name)\n",
    "\n",
    "# Specify which columns to include in the final output\n",
    "output_columns = ['id', 'latitude', 'longitude', 'brand']\n",
    "transformed_data = []  # List to store processed rows\n",
    "\n",
    "# Open and read the input CSV file\n",
    "with open(input_csv_file_path, 'r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "\n",
    "    if reader.fieldnames:  # Check if the file has headers\n",
    "        for row in reader:\n",
    "            original_brand = row.get('brand', '').strip()  # Get and clean up the 'brand' field\n",
    "            modified_brand = original_brand  # Default to original unless we recognize it\n",
    "\n",
    "            # Normalize the brand names\n",
    "            if \"InPost\" in original_brand:\n",
    "                modified_brand = \"Paczkomat InPost\"\n",
    "            elif \"DHL\" in original_brand:\n",
    "                modified_brand = \"DHL BOX 24/7\"\n",
    "            elif \"DPD\" in original_brand:\n",
    "                modified_brand = \"DPD Pickup\"\n",
    "            elif original_brand == \"Allegro One Box\":\n",
    "                modified_brand = \"Allegro One Box\"\n",
    "            elif original_brand == \"Orlen Paczka\":\n",
    "                modified_brand = \"Orlen Paczka\"\n",
    "\n",
    "            # Build a new dictionary with the desired fields\n",
    "            transformed_row = {\n",
    "                'id': row.get('id'),\n",
    "                'latitude': row.get('latitude'),\n",
    "                'longitude': row.get('longitude'),\n",
    "                'brand': modified_brand\n",
    "            }\n",
    "\n",
    "            # Add the cleaned and transformed row to our output list\n",
    "            transformed_data.append(transformed_row)\n",
    "\n",
    "# Write the cleaned data to the output CSV file\n",
    "if transformed_data:\n",
    "    with open(output_csv_file_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=output_columns)\n",
    "        writer.writeheader()           # Write the column headers first\n",
    "        writer.writerows(transformed_data)  # Then write all the cleaned rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b8cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytywanie danych z pliku: C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\final1.csv\n",
      "Usunięto 4 wierszy.\n",
      "Pozostało 19642 wierszy do zapisania.\n",
      "Zapisywanie przefiltrowanych danych do pliku CSV: C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\final2.csv\n",
      "Dane zostały pomyślnie zapisane do C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\final2.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_csv_file_path = r\"C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\final1.csv\"\n",
    "output_csv_folder_path = r\"C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\"\n",
    "output_csv_file_name = \"final2.csv\"\n",
    "output_csv_file_path = os.path.join(output_csv_folder_path, output_csv_file_name)\n",
    "\n",
    "# Define the desired columns in the output\n",
    "output_columns = ['id', 'latitude', 'longitude', 'brand']\n",
    "\n",
    "# Define brand keywords that should be excluded\n",
    "brands_to_remove = [\"Paczkomat\", \"SwipBox\"]\n",
    "\n",
    "# List to store rows that pass the filter\n",
    "filtered_data = []\n",
    "\n",
    "# Open and read the input CSV file\n",
    "with open(input_csv_file_path, 'r', newline='', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "\n",
    "    if reader.fieldnames:  # Ensure file has headers\n",
    "        for row in reader:\n",
    "            current_brand = row.get('brand', '').strip()\n",
    "\n",
    "            # Filter out rows whose brand matches any in brands_to_remove\n",
    "            if current_brand not in brands_to_remove:\n",
    "                filtered_data.append(row)\n",
    "\n",
    "# If any rows passed the filter, write them to the new CSV\n",
    "if filtered_data:\n",
    "    with open(output_csv_file_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=output_columns)\n",
    "        writer.writeheader()           # Write column headers\n",
    "        writer.writerows(filtered_data)  # Write filtered rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84102855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krok 1: Wczytywanie danych o paczkomatach z C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\final2.csv...\n",
      "Wczytano 19446 paczkomatów jako GeoDataFrame.\n",
      "Czas wczytywania paczkomatów: 3.98s.\n",
      "\n",
      "Krok 2: Wczytywanie siatki z C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\better_coords.csv i tworzenie poligonów (2km x 2km)...\n",
      "Wczytano 77273 środków kwadratów z pliku siatki.\n",
      "Tworzenie geometrii poligonów dla kwadratów (to może chwilę potrwać)...\n",
      "Utworzono geometrię dla 5000/77273 kwadratów...\n",
      "Utworzono geometrię dla 10000/77273 kwadratów...\n",
      "Utworzono geometrię dla 15000/77273 kwadratów...\n",
      "Utworzono geometrię dla 20000/77273 kwadratów...\n",
      "Utworzono geometrię dla 25000/77273 kwadratów...\n",
      "Utworzono geometrię dla 30000/77273 kwadratów...\n",
      "Utworzono geometrię dla 35000/77273 kwadratów...\n",
      "Utworzono geometrię dla 40000/77273 kwadratów...\n",
      "Utworzono geometrię dla 45000/77273 kwadratów...\n",
      "Utworzono geometrię dla 50000/77273 kwadratów...\n",
      "Utworzono geometrię dla 55000/77273 kwadratów...\n",
      "Utworzono geometrię dla 60000/77273 kwadratów...\n",
      "Utworzono geometrię dla 65000/77273 kwadratów...\n",
      "Utworzono geometrię dla 70000/77273 kwadratów...\n",
      "Utworzono geometrię dla 75000/77273 kwadratów...\n",
      "Utworzono geometrię dla 77273/77273 kwadratów...\n",
      "Utworzono 77273 poligonów kwadratów jako GeoDataFrame.\n",
      "Czas wczytywania siatki i tworzenia poligonów: 43.20s.\n",
      "\n",
      "Krok 3: Wykonywanie łączenia przestrzennego (spatial join)...\n",
      "Zakończono łączenie przestrzenne. Znaleziono 19315 paczkomatów w kwadratach.\n",
      "Czas łączenia przestrzennego: 0.09s.\n",
      "\n",
      "Krok 4: Agregacja i zliczanie paczkomatów według brandu w każdym kwadracie...\n",
      "Grupowanie po: 'grid_id', zliczanie unikalnych wartości w kolumnie 'id' dla każdego brandu.\n",
      "Zakończono agregację.\n",
      "Czas agregacji: 0.04s.\n",
      "\n",
      "Krok 5: Zapisywanie wyników do C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\grid_2km_with_locker_counts.csv...\n",
      "Wyniki zapisano pomyślnie do C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\\grid_2km_with_locker_counts.csv\n",
      "Czas zapisu: 0.68s.\n",
      "\n",
      "Całkowity czas wykonania skryptu: 48.00 sekund.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas\n",
    "from shapely.geometry import Point, Polygon\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Ścieżki do plików wejściowych i wyjściowych\n",
    "base_path = r\"C:\\Users\\anton\\OneDrive\\Pulpit\\Mastercard\"\n",
    "grid_file_path = os.path.join(base_path, \"better_coords.csv\")  # Plik z punktami siatki\n",
    "lockers_file_path = os.path.join(base_path, \"final2.csv\")      # Plik z paczkomatami\n",
    "output_file_path = os.path.join(base_path, \"grid_2km_with_locker_counts.csv\")  # Wynik\n",
    "\n",
    "# Stałe do przeliczania km na stopnie geograficzne\n",
    "HALF_SIDE_KM = 1.0  # Połowa boku kwadratu (pełny bok = 2 km)\n",
    "KM_PER_DEGREE_LAT = 111.132  # Stała odległość km/° szerokości\n",
    "KM_PER_DEGREE_LON_BASE = 111.320  # Bazowa wartość dla długości geograficznej\n",
    "\n",
    "# Funkcja do przeliczenia km na stopnie szerokości\n",
    "def calculate_delta_lat_degrees(km):\n",
    "    return km / KM_PER_DEGREE_LAT\n",
    "\n",
    "# Funkcja do przeliczenia km na stopnie długości (z uwzględnieniem szerokości geograficznej)\n",
    "def calculate_delta_lon_degrees(km, center_lat_degrees):\n",
    "    center_lat_radians = math.radians(center_lat_degrees)\n",
    "    cos_lat = math.cos(center_lat_radians)\n",
    "    if abs(cos_lat) < 1e-9:\n",
    "        return float('inf')\n",
    "    km_per_degree_lon_at_lat = KM_PER_DEGREE_LON_BASE * cos_lat\n",
    "    if km_per_degree_lon_at_lat == 0:\n",
    "        return float('inf')\n",
    "    return km / km_per_degree_lon_at_lat\n",
    "\n",
    "# Krok 1: Wczytanie danych o paczkomatach\n",
    "print(\"Krok 1: Wczytywanie paczkomatów...\")\n",
    "lockers_df = pd.read_csv(lockers_file_path, usecols=['id', 'latitude', 'longitude', 'brand'], dtype={'id': str})\n",
    "lockers_df.dropna(subset=['latitude', 'longitude', 'brand'], inplace=True)\n",
    "lockers_df['brand'] = lockers_df['brand'].astype(str).str.strip()\n",
    "lockers_df = lockers_df[lockers_df['brand'] != '']\n",
    "lockers_df['latitude'] = pd.to_numeric(lockers_df['latitude'], errors='coerce')\n",
    "lockers_df['longitude'] = pd.to_numeric(lockers_df['longitude'], errors='coerce')\n",
    "lockers_df.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "\n",
    "# Konwersja paczkomatów na obiekty geometryczne typu Point\n",
    "geometry_lockers = [Point(xy) for xy in zip(lockers_df['longitude'], lockers_df['latitude'])]\n",
    "lockers_gdf = geopandas.GeoDataFrame(lockers_df, geometry=geometry_lockers, crs=\"EPSG:4326\")\n",
    "print(f\"Wczytano {len(lockers_gdf)} paczkomatów.\")\n",
    "\n",
    "# Jeśli brak paczkomatów – zakończ program\n",
    "if lockers_gdf.empty:\n",
    "    print(\"Brak danych o paczkomatach. Przerywam.\")\n",
    "    exit()\n",
    "\n",
    "# Krok 2: Wczytanie punktów siatki\n",
    "print(\"Krok 2: Wczytywanie siatki i tworzenie poligonów...\")\n",
    "grid_df_cols_to_use = ['longitude', 'latitude', 'geometry']\n",
    "grid_df = pd.read_csv(grid_file_path, usecols=grid_df_cols_to_use)\n",
    "grid_df.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "grid_df['latitude'] = pd.to_numeric(grid_df['latitude'], errors='coerce')\n",
    "grid_df['longitude'] = pd.to_numeric(grid_df['longitude'], errors='coerce')\n",
    "grid_df.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "grid_df.reset_index(drop=True, inplace=True)\n",
    "grid_df['grid_id'] = grid_df.index  # Nadanie unikalnego ID każdemu punktowi siatki\n",
    "\n",
    "# Jeśli brak punktów siatki – zakończ program\n",
    "if grid_df.empty:\n",
    "    print(\"Brak danych siatki. Przerywam.\")\n",
    "    exit()\n",
    "\n",
    "# Tworzenie kwadratowych poligonów wokół każdego punktu siatki\n",
    "print(f\"Wczytano {len(grid_df)} środków kwadratów. Tworzenie poligonów...\")\n",
    "grid_polygons = []\n",
    "total_grids = len(grid_df)\n",
    "for index, row in grid_df.iterrows():\n",
    "    center_lon = row['longitude']\n",
    "    center_lat = row['latitude']\n",
    "    delta_lat_deg = calculate_delta_lat_degrees(HALF_SIDE_KM)\n",
    "    delta_lon_deg = calculate_delta_lon_degrees(HALF_SIDE_KM, center_lat)\n",
    "\n",
    "    # Oblicz rogi kwadratu 2x2km\n",
    "    min_lon, max_lon = center_lon - delta_lon_deg, center_lon + delta_lon_deg\n",
    "    min_lat, max_lat = center_lat - delta_lat_deg, center_lat + delta_lat_deg\n",
    "\n",
    "    # Utwórz poligon kwadratu\n",
    "    polygon = Polygon([(min_lon, min_lat), (max_lon, min_lat), (max_lon, max_lat), (min_lon, max_lat), (min_lon, min_lat)])\n",
    "    grid_polygons.append(polygon)\n",
    "\n",
    "    # Wydruk postępu\n",
    "    if (index + 1) % 10000 == 0 or (index + 1) == total_grids:\n",
    "        print(f\"Utworzono geometrię dla {index + 1}/{total_grids} kwadratów...\")\n",
    "\n",
    "# Utworzenie GeoDataFrame z siatki\n",
    "grid_gdf = geopandas.GeoDataFrame(grid_df, geometry=grid_polygons, crs=\"EPSG:4326\")\n",
    "print(f\"Utworzono {len(grid_gdf)} poligonów.\")\n",
    "\n",
    "# Krok 3: Łączenie przestrzenne (spatial join) – sprawdzenie, które paczkomaty mieszczą się w którym kwadracie\n",
    "print(\"Krok 3: Łączenie przestrzenne...\")\n",
    "start_sjoin_time = time.time()\n",
    "joined_gdf = geopandas.sjoin(lockers_gdf, grid_gdf, how=\"inner\", predicate=\"within\", lsuffix=\"locker\", rsuffix=\"grid\")\n",
    "print(f\"Łączenie przestrzenne zakończone w {time.time() - start_sjoin_time:.2f}s. Znaleziono {len(joined_gdf)} paczkomatów w kwadratach.\")\n",
    "\n",
    "# Krok 4: Agregacja wyników – zliczanie paczkomatów w każdym kwadracie wg marki\n",
    "if joined_gdf.empty:\n",
    "    # Brak trafień – utwórz pusty DataFrame z zerami\n",
    "    result_df = grid_df.copy()\n",
    "    unique_brands_from_lockers = [str(b).strip() for b in lockers_gdf['brand'].unique() if str(b).strip()]\n",
    "    for brand_name in unique_brands_from_lockers:\n",
    "        result_df[brand_name] = 0\n",
    "else:\n",
    "    print(\"Krok 4: Agregacja...\")\n",
    "    group_column = 'grid_id'\n",
    "    count_column = 'id'\n",
    "    \n",
    "    # Grupowanie po grid_id i brand, liczenie liczby paczkomatów\n",
    "    counts_df = joined_gdf.groupby([group_column, 'brand'])[count_column].count().unstack(fill_value=0)\n",
    "    \n",
    "    # Połączenie z oryginalną siatką\n",
    "    result_df = grid_df.merge(counts_df, on='grid_id', how='left')\n",
    "    \n",
    "    # Uzupełnianie brakujących kolumn o zera\n",
    "    brand_columns_to_fill = [str(b).strip() for b in lockers_gdf['brand'].unique() if str(b).strip()]\n",
    "    for col in brand_columns_to_fill:\n",
    "        if col not in result_df.columns:\n",
    "            result_df[col] = 0\n",
    "        else:\n",
    "            result_df[col] = result_df[col].fillna(0).astype(int)\n",
    "    print(\"Agregacja zakończona.\")\n",
    "\n",
    "# Krok 5: Zapis wyników do pliku CSV\n",
    "print(\"Krok 5: Zapisywanie wyników...\")\n",
    "cols_to_save = list(result_df.columns)\n",
    "\n",
    "# Jeśli kolumna 'geometry' zawiera obiekty zamiast tekstu – nie zapisujemy jej\n",
    "if 'geometry' in cols_to_save and not all(isinstance(g, str) for g in result_df['geometry'].dropna()):\n",
    "    cols_to_save.remove('geometry')\n",
    "\n",
    "result_df[cols_to_save].to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "print(f\"Wyniki zapisano do {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
