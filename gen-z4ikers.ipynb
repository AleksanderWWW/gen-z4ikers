{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57014eff-dad6-407a-8d44-13bad9a8d11d",
   "metadata": {},
   "source": [
    "# SGH x Mastercard Hackathon - May 2025\n",
    "\n",
    "## Fraud detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef30adb-190d-420e-ac78-ab55276c563a",
   "metadata": {},
   "source": [
    "Let's start by setting up the project and installing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2456d84-4a2d-419d-990b-f83bf0bcca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c02192-0831-4478-adcb-214cc3a2ce6f",
   "metadata": {},
   "source": [
    "Next, we import the packages we'll need for data processing, feature engineering, model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66defe1-26a3-42ee-9e10-0994f1a5087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import joblib\n",
    "from typing import Callable, TypeVar, ParamSpec\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e90192-a958-4605-87d5-cf713112c0ca",
   "metadata": {},
   "source": [
    "## Step 1. Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6eae3b-be08-402a-8bbe-4fb4a83d96f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = TypeVar(\"R\")\n",
    "P = ParamSpec(\"P\")\n",
    "\n",
    "# to obtain cleaner stack trace if wrong path is passed\n",
    "def with_file_existence_checked(func: Callable[P, R]) -> Callable[P, R]:\n",
    "    def _inner(file_path: Path, *args: P.args, **kwargs: P.kwargs) -> R:\n",
    "        if not file_path.is_file():\n",
    "            raise ValueError(f\"file {file_path} doesn't exist\")\n",
    "\n",
    "        return func(file_path, *args, **kwargs)\n",
    "    return _inner\n",
    "\n",
    "\n",
    "@with_file_existence_checked\n",
    "def read_transactions(transaction_path: Path) -> pd.DataFrame:\n",
    "    data_transactions = pd.read_json(transaction_path, lines=True)\n",
    "    location_df = pd.json_normalize(data_transactions['location'])\n",
    "    location_df.columns = ['latitude', 'longitude']\n",
    "    \n",
    "    return data_transactions.drop(columns=['location']).join(location_df)\n",
    "\n",
    "\n",
    "@with_file_existence_checked\n",
    "def read_merchant_data(merchant_path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(merchant_path)\n",
    "\n",
    "\n",
    "@with_file_existence_checked\n",
    "def read_user_data(user_path: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(user_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591db8bb-c7ac-448b-ae83-ee15b02a386b",
   "metadata": {},
   "source": [
    "Once the files are loaded we can proceed to merge them together into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a1097-dd26-45dd-9ce6-2f9181af661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_dataset(transaction_data: pd.DataFrame, merchant_data: pd.DataFrame, user_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    return transaction_data.merge(merchant_data, on='merchant_id', how='left').merge(user_data, on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c57b5-5ec7-42b1-b3fb-c38d78412e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data() -> pd.DataFrame():\n",
    "    data_transactions = read_transactions(Path(\"data/transactions.json\").resolve())\n",
    "    data_merchants = read_merchant_data(Path(\"data/merchants.csv\").resolve())\n",
    "    data_users = read_user_data(Path(\"data/users.csv\").resolve())\n",
    "    \n",
    "    return get_raw_dataset(data_transactions, data_merchants, data_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8384a0-06e2-4fc2-ab87-dea6094d5415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ingest_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360b32c-4b50-462a-9105-38b69c536b94",
   "metadata": {},
   "source": [
    "## Step 2. Data pre-processing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a5faee-68d9-4cff-a04d-b186e6c83c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_execution(func: Callable[P, R]) -> Callable[P, R]:\n",
    "    def _inner(*args: P.args, **kwargs: P.kwargs) -> R:\n",
    "        start = time.perf_counter()\n",
    "        print(f\"Executing transformation {func.__name__}\")\n",
    "        result = func(*args, **kwargs)\n",
    "        print(f\"Transformation {func.__name__} executed in {time.perf_counter() - start} seconds.\")\n",
    "        return result\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c5d83-82e3-4a20-9998-3584ce15254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sort transactions by user and timestamp\n",
    "@log_execution\n",
    "def sort_by_user_id_and_timestamp(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.sort_values(by=[\"user_id\", \"timestamp\"])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# 2. Add a binary column indicating whether the user has committed fraud before\n",
    "@log_execution\n",
    "def add_user_fraud_history(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['temp_cumulative_fraud'] = df.groupby('user_id')['is_fraud'].cumsum()\n",
    "    shifted = df.groupby('user_id')['temp_cumulative_fraud'].shift(1)\n",
    "    df['user_has_fraud_history'] = shifted.fillna(0).gt(0).astype(int)\n",
    "    return df.drop(columns=['temp_cumulative_fraud'])\n",
    "\n",
    "# 3. Add a column counting number of transactions by the user\n",
    "@log_execution\n",
    "def add_user_transaction_count(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"dummy\"] = 1\n",
    "    df[\"users_transaction_count\"] = df.groupby(\"user_id\")[\"dummy\"].cumsum().astype(int)\n",
    "    return df.drop(columns=[\"dummy\"])\n",
    "\n",
    "# 4. Add all user-based temporal and statistical features\n",
    "@log_execution\n",
    "def add_user_time_and_amount_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['time_since_last_transaction_user'] = df.groupby('user_id')['timestamp'].diff().dt.days\n",
    "    df['time_since_last_transaction_user'] = df['time_since_last_transaction_user'].fillna(0).astype(int)\n",
    "    \n",
    "    # Amount deviation from user history\n",
    "    df['transaction_amount_std_from_user_mean'] = (\n",
    "        df.groupby('user_id')['amount']\n",
    "        .transform(lambda x: (x.expanding().mean().shift(1) - x) / x.expanding().std().shift(1))\n",
    "    )\n",
    "    df['transaction_amount_std_from_user_mean'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df['transaction_amount_std_from_user_mean'].fillna(0, inplace=True)\n",
    "\n",
    "    # Time-based features\n",
    "    df['signup_date'] = pd.to_datetime(df['signup_date'])\n",
    "    df['transaction_date'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    df['days_since_signup'] = (df['transaction_date'] - df['signup_date']).dt.total_seconds() // (24 * 3600)\n",
    "    df['days_since_signup'] = df['days_since_signup'].astype(int)\n",
    "\n",
    "    df['transaction_hour'] = df['transaction_date'].dt.hour\n",
    "    df['is_transaction_night'] = ((df['transaction_hour'] >= 0) & (df['transaction_hour'] < 6)).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 5. Add rolling transaction counts in the last N hours for each user\n",
    "@log_execution\n",
    "def add_rolling_transaction_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['transactions_last_1h_user'] = (\n",
    "        df.groupby('user_id', group_keys=False)\n",
    "        .apply(lambda g: rolling_count(g, '1h'))\n",
    "        .sort_index()\n",
    "    )\n",
    "    df['transactions_last_24h_user'] = (\n",
    "        df.groupby('user_id', group_keys=False)\n",
    "        .apply(lambda g: rolling_count(g, '24h'))\n",
    "        .sort_index()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# 6. Add moving average of user's past transaction amounts\n",
    "@log_execution\n",
    "def add_avg_amount_last_5_tx(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['avg_amount_last_5_tx_user'] = (\n",
    "        df.groupby('user_id')['amount']\n",
    "        .transform(lambda x: x.rolling(window=5, min_periods=1).mean().shift(1))\n",
    "    )\n",
    "    df['avg_amount_last_5_tx_user'].fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "# 7. Add merchant fraud history\n",
    "@log_execution\n",
    "def add_merchant_fraud_history(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.sort_values(by=[\"merchant_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "    df['temp_cumulative_fraud'] = df.groupby('merchant_id')['is_fraud'].cumsum()\n",
    "    shifted = df.groupby('merchant_id')['temp_cumulative_fraud'].shift(1)\n",
    "    df['merchant_has_fraud_history'] = shifted.fillna(0).gt(0).astype(int)\n",
    "    return df.drop(columns=['temp_cumulative_fraud'])\n",
    "\n",
    "# 8. Add binary indicator whether user changed device from their usual one\n",
    "@log_execution\n",
    "def add_device_change_feature(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['user_mode_device_temp'] = df.groupby('user_id')['device'].transform(_get_user_device_mode_for_transform)\n",
    "    has_mode = df['user_mode_device_temp'].notna()\n",
    "    df['device_changed'] = 0\n",
    "    df.loc[has_mode, 'device_changed'] = (\n",
    "        (df.loc[has_mode, 'device'] != df.loc[has_mode, 'user_mode_device_temp']).astype(int)\n",
    "    )\n",
    "    return df.drop(columns=['user_mode_device_temp'])\n",
    "\n",
    "# Helper function for rolling count\n",
    "def rolling_count(series, window):\n",
    "    # series is a DataFrame with 'transaction_date'\n",
    "    return series['transaction_date'].apply(\n",
    "        lambda x: ((series['transaction_date'] >= (x - pd.Timedelta(window))) & (series['transaction_date'] < x)).sum()\n",
    "    )\n",
    "\n",
    "def _get_user_device_mode_for_transform(series):\n",
    "    # Ensure numpy is imported in your notebook, e.g., import numpy as np\n",
    "    modes = series.mode()\n",
    "    if not modes.empty:\n",
    "        return modes.iloc[0]\n",
    "    return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8f390-ec88-48d7-be46-7167bd0fe405",
   "metadata": {},
   "outputs": [],
   "source": [
    "@log_execution\n",
    "def add_temporal_pattern_features(df_cleaned: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"1. Temporal Pattern Features\"\"\"\n",
    "    df_cleaned['day_of_week'] = df_cleaned['transaction_date'].dt.dayofweek\n",
    "    df_cleaned['is_weekend'] = (df_cleaned['transaction_date'].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "    df_cleaned = df_cleaned.sort_values(by=[\"user_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "    df_cleaned['user_tx_velocity_15m'] = (\n",
    "        df_cleaned.groupby('user_id', group_keys=False)\n",
    "        .apply(lambda group: rolling_count(group, '15m'))\n",
    "        .sort_index()\n",
    "    ).astype(int)\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "@log_execution\n",
    "def add_high_risk_merchant_features(df_cleaned: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"2. High-Risk Merchant Features\"\"\"\n",
    "    df_cleaned = df_cleaned.sort_values(by=[\"merchant_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    df_cleaned['merchant_past_tx_count'] = df_cleaned.groupby('merchant_id').cumcount()\n",
    "    df_cleaned['merchant_past_fraud_count'] = (\n",
    "        df_cleaned.groupby('merchant_id')['is_fraud']\n",
    "        .transform(lambda x: x.shift(1).expanding().sum())\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    df_cleaned['merchant_past_fraud_rate'] = (\n",
    "        df_cleaned['merchant_past_fraud_count'] /\n",
    "        (df_cleaned['merchant_past_tx_count'] + 1e-6)\n",
    "    ).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    df_cleaned['merchant_avg_amount_hist'] = (\n",
    "        df_cleaned.groupby('merchant_id')['amount']\n",
    "        .transform(lambda x: x.shift(1).expanding().mean())\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    df_cleaned['amount_vs_merchant_avg_ratio'] = (\n",
    "        df_cleaned['amount'] / (df_cleaned['merchant_avg_amount_hist'] + 1e-6)\n",
    "    ).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "@log_execution\n",
    "def add_cardholder_anomaly_features(df_cleaned: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"3. Cardholder Behavior Anomaly Features\"\"\"\n",
    "    df_cleaned = df_cleaned.sort_values(by=[\"user_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    if 'avg_amount_last_5_tx_user' in df_cleaned.columns:\n",
    "        df_cleaned['amount_vs_user_avg_5tx_ratio'] = (\n",
    "            df_cleaned['amount'] / (df_cleaned['avg_amount_last_5_tx_user'] + 1e-6)\n",
    "        ).replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    else:\n",
    "        print(\"Warning: Column 'avg_amount_last_5_tx_user' not found. Skipping 'amount_vs_user_avg_5tx_ratio' feature.\")\n",
    "\n",
    "    df_cleaned['is_new_merchant_for_user'] = (\n",
    "        df_cleaned.groupby(['user_id', 'merchant_id']).cumcount() == 0\n",
    "    ).astype(int)\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "@log_execution\n",
    "def finalize_feature_processing(df_cleaned: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Final sort and column cleanup\"\"\"\n",
    "    df_cleaned = df_cleaned.sort_values(by=[\"user_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "    df_cleaned = df_cleaned.drop(columns=['transaction_date', 'signup_date'])\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce21d1f-b38a-4937-8b7e-a1bec9344777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data[\"education\"] = data[\"education\"].fillna(\"No Education\")\n",
    "\n",
    "    df_cleaned = data.drop(columns = [\"transaction_id\", \"currency\"])\n",
    "\n",
    "    transformations = (\n",
    "        sort_by_user_id_and_timestamp,\n",
    "        add_user_fraud_history,\n",
    "        add_user_transaction_count,\n",
    "        add_user_time_and_amount_features,\n",
    "        add_rolling_transaction_counts,\n",
    "        add_avg_amount_last_5_tx,\n",
    "        add_merchant_fraud_history,\n",
    "        add_device_change_feature,\n",
    "        add_temporal_pattern_features,\n",
    "        add_high_risk_merchant_features,\n",
    "        add_cardholder_anomaly_features,\n",
    "        finalize_feature_processing,\n",
    "    )\n",
    "\n",
    "    with tqdm(desc=\"data_preprocessing...\", total=len(transformations)) as pbar:\n",
    "        for func in transformations:\n",
    "            df_cleaned = func(df_cleaned)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return df_cleaned      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef6c8f-f653-4f25-97b4-129a4df3cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790459f0-f515-427e-bfaa-e3e4d414a7c0",
   "metadata": {},
   "source": [
    "## Step 3. Prepare train-test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8736b7-c5f3-44b7-bff9-13737164db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataSetRepo:\n",
    "    X_train: pd.DataFrame\n",
    "    y_train: pd.Series\n",
    "    X_test: pd.DataFrame\n",
    "    y_test: pd.Series\n",
    "    \n",
    "\n",
    "def prepare_datasets(df: pd.DataFrame) -> DataSetRepo:\n",
    "    enc = OrdinalEncoder()\n",
    "\n",
    "    X = df.drop(columns=[\"is_fraud\"])\n",
    "    y = df[\"is_fraud\"]\n",
    "    \n",
    "    X = pd.DataFrame(enc.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    X_train_orig, X_test, y_train_orig, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Undersample only the training data\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train, y_train = rus.fit_resample(X_train_orig, y_train_orig)\n",
    "\n",
    "    return DataSetRepo(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac298f5b-e484-4233-9260-1f962d0d3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = prepare_datasets(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b206bc-041b-4a9e-a195-2ee89fb47f25",
   "metadata": {},
   "source": [
    "## Step 4. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8b3b2-9386-4db0-80ae-21592200a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cost_function(y_true, y_pred, cost_fn=10, cost_fp=3):\n",
    "    y_true_arr = np.asarray(y_true)\n",
    "    y_pred_arr = np.asarray(y_pred)\n",
    "    fn_count = np.sum((y_true_arr == 1) & (y_pred_arr == 0))\n",
    "    fp_count = np.sum((y_true_arr == 0) & (y_pred_arr == 1))\n",
    "    return fn_count * cost_fn + fp_count * cost_fp\n",
    "\n",
    "custom_scorer = make_scorer(custom_cost_function, greater_is_better=False)\n",
    "\n",
    "# Recursive feature elimination with CV, supporting custom scorer\n",
    "\n",
    "def rfe_with_cv(\n",
    "    model_prototype,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    n_splits: int = 3,\n",
    "    scoring=custom_scorer,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    features = X.columns.tolist()\n",
    "    best_score = np.inf  # lower cost is better\n",
    "    best_features = features.copy()\n",
    "    history = {}\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    while features:\n",
    "        model = clone(model_prototype)\n",
    "        scores = cross_val_score(\n",
    "            model,\n",
    "            X[features],\n",
    "            y,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        mean_score = np.mean(scores)\n",
    "        history[len(features)] = mean_score\n",
    "\n",
    "        if mean_score < best_score or (\n",
    "            np.isclose(mean_score, best_score) and len(features) < len(best_features)\n",
    "        ):\n",
    "            best_score = mean_score\n",
    "            best_features = features.copy()\n",
    "\n",
    "        if len(features) == 1:\n",
    "            break\n",
    "\n",
    "        model_full = clone(model_prototype)\n",
    "        model_full.fit(X[features], y)\n",
    "        importances = pd.Series(\n",
    "            model_full.feature_importances_,\n",
    "            index=features\n",
    "        )\n",
    "        least_imp = importances.idxmin()\n",
    "        features.remove(least_imp)\n",
    "\n",
    "    print(f\"RFE complete: lowest cost={best_score:.2f} with {len(best_features)} features.\")\n",
    "    return best_features, history\n",
    "\n",
    "# Run RFE on balanced training data\n",
    "xgb_prototype = XGBRFClassifier()\n",
    "best_feats, rfe_history = rfe_with_cv(xgb_prototype, repo.X_train, repo.y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f5236c-c58e-416d-8956-ba063f4f30b3",
   "metadata": {},
   "source": [
    "## Step 5. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c54aea-3619-4133-a787-1feba4e72026",
   "metadata": {},
   "source": [
    "We will use `optuna` to search the hyperparameter space for configuration yielding optimal results \n",
    "for an XGBOOST classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890c991-79fe-4bad-8249-95f487b17222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the RFE-selected features\n",
    "X_rfe = repo.X_train[best_feats]\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "        \"n_jobs\": -1,\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"use_label_encoder\": False,\n",
    "    }\n",
    "    model = XGBRFClassifier(**params)\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    # Return mean custom cost (lower is better)\n",
    "    scores = cross_val_score(model, X_rfe, repo.y_train, cv=cv, scoring=custom_scorer, n_jobs=-1)\n",
    "    return np.mean(scores)\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)\n",
    "\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Lowest cost:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aba6bf-b23c-490e-a97c-780a332197c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = XGBRFClassifier(**study.best_params)\n",
    "final_model.fit(repo.X_train[best_feats], repo.y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dea8c7-8994-4a86-9b19-d093d5a3980d",
   "metadata": {},
   "source": [
    "## Step 6. Evaluate model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6cd436-1fb8-44eb-9c04-987e6004d13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(final_model.feature_importances_, index=best_feats)\n",
    "importances.sort_values().plot(kind='barh', figsize=(6,8))\n",
    "plt.title(\"XGBoost Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ec6b38-d3b2-4607-96d4-51c47ec2b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_model.predict(repo.X_test[best_feats])\n",
    "\n",
    "cm = confusion_matrix(repo.y_test, y_pred)\n",
    "\n",
    "precision = precision_score(repo.y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(repo.y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(repo.y_test, y_pred, zero_division=0)\n",
    "\n",
    "# Define class labels for the plot\n",
    "class_labels = ['Not Fraud', 'Fraud']\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 7)) # Increased figure height slightly to accommodate metrics\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels,\n",
    "            annot_kws={\"size\": 14}) # Makes annotations easier to read\n",
    "\n",
    "plt.title('Confusion Matrix - Final XGBoost Model (Tuned + RFE features)', fontsize=16, y=1.08) # Adjust title position\n",
    "plt.ylabel('Actual Class', fontsize=14)\n",
    "plt.xlabel('Predicted Class', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Adding precision, recall, and F1-score text below the heatmap\n",
    "metrics_text = f\"Precision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1-score: {f1:.4f}\"\n",
    "plt.text(0.5, -0.15, metrics_text, ha='center', va='center', transform=plt.gca().transAxes, fontsize=12,\n",
    "         bbox=dict(boxstyle='round,pad=0.5', fc='aliceblue', ec='lightsteelblue', lw=1))\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95]) # Adjust layout to prevent labels/text from overlapping, added bottom margin\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb7f17-0b03-48f4-b951-c90932f4e262",
   "metadata": {},
   "source": [
    "## Save the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531fea5e-9a2e-45cc-b1b8-808f3f2e8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(final_model, Path(\".\") / \"model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
